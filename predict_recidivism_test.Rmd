---
title: "predict_recidivism"
author: "Beau Coker"
date: "11/5/2018"
output: html_document
---


**NOTE: MUST CREATE HELD OUT TEST SET

Here, we use out-of-box ML methods to predict recidivism 
```{r include = FALSE}
library(tidyverse)
library(magrittr)
library(pROC)
source('util.R')
load("expanded_features.Rdata")
load("compas_psa.Rdata")
```

### Prepare data 

```{r}
### Add useful columns to features and apply row filters used for all models
features_filt = features_before_on %>%
  inner_join(
    data_before %>% 
      select(person_id, screening_date, people) %>%
      unnest() %>%
      select(person_id, screening_date, race, sex, name),
    by = c("person_id","screening_date")
  ) %>%
  inner_join(features_on, by = c("person_id","screening_date")) %>%
  inner_join(
    psa_features%>%
      select(-c(p_current_age,p_prison)), 
    by = c("person_id","screening_date"))%>%
  inner_join(outcomes, by = c("person_id","screening_date")) %>%
  filter(`Risk of Recidivism_decile_score` != -1, `Risk of Violence_decile_score` != -1) %>% # Filter 1
  filter(!is.na(current_offense_date)) %>% # Filter 3
  filter(screening_date <= current_offense_date_limit) %>% # Filter 4
  mutate(recid_use = as.factor(recid), # Select recidivism or violent recidivism to use in this script 
         decile_use = `Risk of Recidivism_decile_score`) # Select recidivism or violent recidivism decile score to use in this script

## Select features and round count features
train = features_filt %>%
  transmute(
    #COMPAS Risk of Recidivism Features
    p_current_age,
    p_age_first_offense,
    p_charge,
    p_jail30 = pmin(p_jail30,5),
    p_prison = pmin(p_prison,5),
    p_probation = pmin(p_probation,5),
    
    #COMPAS Risk of violent recidivism features
    p_juv_fel_count,
    p_felprop_violarrest,
    p_murder_arrest,
    p_felassault_arrest,
    p_misdemassault_arrest,
    p_famviol_arrest,
    p_sex_arrest,
    p_weapons_arrest,
    
    #Misc Features
    p_arrest,
    p_property,
    p_traffic,
    p_drug,
    p_dui,
    p_domestic,
    p_stalking,
    p_voyeurism,
    p_fraud,
    p_stealing,
    p_trespass,
    recid_use)

# train = train[1:100,] # Comment for full training set. This is just for testing.
```


### Train xgboost

```{r include = FALSE}
library(xgboost)
```


```{r}
## Format for xgboost
train_xgb = xgb.DMatrix(
  "data" = train %>% select(-recid_use) %>% as.matrix(),
  "label" = train %>% select(recid_use) %>% as.matrix()
)

# Specify each parameter as a single value or a vector of values
# Each combination will be tested
param_xgb <- list(
  objective = "binary:logistic",
  eta = c(.05,.1),
  gamma = c(.5), 
  max_depth = c(2),
  min_child_weight = c(5),
  subsample = c(1),
  colsample_bytree = c(1),
  early_stopping_rounds=50
)

param_xgb = expand.grid(param_xgb) # Each row is a set of parameters to be cross validated

# Only one value for each of these parameters is allowed
setup_xgb = list(
  nrounds=10000,
  nfold=5
)
```

```{r}
set.seed(2812)
out_xgb = fit_xgb_auc(train_xgb, param_xgb, setup_xgb)
out_xgb$performance
out_xgb$roc
```

### Train BART

```{r include = FALSE}
options(java.parameters = "-Xmx2g") 
library(bartMachine)
```


```{r}
param_bart = expand.grid(list(
  num_trees = c(50, 75),
  alpha = 0.95, 
  beta = 2, 
  k = c(2), 
  q = 0.9, 
  nu = 3)
)

setup_bart = list(
  nfold=2
)

out_bart = train %>%
  mutate(y = as.factor(recid_use)) %>%
  select(-recid_use) %>%
  fit_bart_auc(param_bart,setup_bart)
```
  i_param seed train_auc_mean train_auc_std test_auc_mean test_auc_std
1       1 5908      0.7317067    0.01156313     0.7041193   0.01156313
2       2 8371      0.7340461    0.01171209     0.7064116   0.01171209
```{r}
out_bart$performance
plot(out_bart$roc, percent = F, 
            ci.alpha = .9, stratified = F,
            reuse.auc = T, print.auc = T, ci = T, ci.type = "bars",
            print.thres.cex = .7,
            main = paste("ROC curve using N = ", nrow(train)),
            smooth =T, show.thres = T, legacy.axes = T,
            plot = T, grid =T )
```


```{r}
plot_convergence_diagnostics(out_bart$mdl_best)
```

```{r}
investigate_var_importance(out_bart$mdl_best, num_replicates_for_avg = 20)
```


### Train logistic regression classifier
train_auc_mean train_auc_std test_auc_mean test_auc_std
1      0.7034106   0.004943927     0.6955958  0.004943927
```{r include = FALSE}
setup_glm = list(
  nfold=5
)

out_glm = train %>%
          mutate(y = as.factor(recid_use)) %>%
          select(-recid_use) %>%
          fit_glm_auc(setup_glm)

out_glm$performance
plot(out_glm$roc,
     print.auc = T,
     main = paste("ROC curve using N = ", nrow(train)),
     legacy.axes = T,
     grid =T )
# print("accuracy is:  ")
# 1-mean(if_else(predict(out_glm$mdl_glm, type='response', newdata=train)>.5,1,0) != train$recid_use)
```

### Train LASSO 
  seed train_auc_mean train_auc_std test_auc_mean test_auc_std
1 8363      0.7011165    0.00243117     0.6962465   0.00243117
```{r include = FALSE}
library(glmnet)
setup_lasso = list(
  nfold=5
)
```

```{r}
out_lasso = train %>%
          mutate(y = as.factor(recid_use)) %>%
          select(-recid_use) %>%
          fit_lasso_auc(setup_lasso)

out_lasso$performance
plot(out_lasso$roc,
     print.auc = T,
     main = paste("ROC curve using N = ", nrow(train)),
     legacy.axes = T,
     grid =T )

```


### Train random forest
 seed train_auc_mean train_auc_std test_auc_mean test_auc_std
1  784      0.8856112   0.003304293     0.6066994  0.003304293

```{r include = FALSE}
library(randomForest)
setup = list(
  nfold=5, 
  seed = 784
)

```


```{r}
out_rf =  train %>%
          mutate(y = as.factor(recid_use)) %>%
          select(-recid_use) %>%
          fit_rf_auc(setup_rf)

out_rf$performance
plot(out_rf$roc,
     print.auc = T,
     main = paste("ROC curve using N = ", nrow(train)),
     legacy.axes = T,
     grid =T )

```

### Train CART
  i_param seed train_auc_mean train_auc_std test_auc_mean test_auc_std
1       1 5163      0.6135910    0.01825666     0.6048626   0.01825666
2       2 6402      0.5416409    0.05702557     0.5361078   0.05702557
3       3 4991      0.6135910    0.01825666     0.6048626   0.01825666
4       4 1511      0.5416409    0.05702557     0.5361078   0.05702557
5       5 3526      0.6135910    0.01825666     0.6048626   0.01825666
6       6 1592      0.5416409    0.05702557     0.5361078   0.05702557
7       7  181      0.6135910    0.01825666     0.6048626   0.01825666
8       8  805      0.5416409    0.05702557     0.5361078   0.05702557
```{r include = FALSE}
library(rpart)
setup_cart = list(
  nfold=5
)

param_cart = expand.grid(list(
  cp=c(.01, .05), 
  minsplit = c(20,40), 
  maxdepth = c(15,30)
  )
)
```


```{r}
out_cart =  train %>%
          mutate(y = as.factor(recid_use)) %>%
          select(-recid_use) %>%
          fit_cart_auc(param_cart, setup_cart)

out_cart$performance
plot(out_cart$roc,
     print.auc = T,
     main = paste("ROC curve using N = ", nrow(train)),
     legacy.axes = T,
     grid =T )

```



### Train SVM
0.7059543 accuracy
```{r}
param_svm = list(
  type = 'C-classification',
  cost = c(0.5,1,2),
  epsilon = .1, # This parameter isn't used for classification but need to set anyway or it will break
  gamma_scale = c(0.5,1,2)
)

```


```{r}
# mdl_svm =fit_svm(recid_use ~ ., train, param_svm)

out_svm = train %>%
          mutate(y = as.factor(recid_use)) %>%
          select(-recid_use) %>%
          fit_svm(recid_use ~ ., train, param_svm)

out_svm$performance
plot(out_svm$roc,
     print.auc = T,
     main = paste("ROC curve using N = ", nrow(train)),
     legacy.axes = T,
     grid =T )


# print("Accuracy is: ")
# 1-mean(mdl_svm$fitted != train$recid_use) # ADJUST GROUP

```

