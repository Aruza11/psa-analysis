---
title: "predict_recidivism"
author: "Beau Coker"
date: "11/5/2018"
output: html_document
---

Here, we use out-of-box ML methods to predict recidivism (currently, xgBoost)
```{r}
library(tidyverse)
library(magrittr)
source('util.R')
load("expanded_features.Rdata")
load("compas_psa.Rdata")
```

### Prepare data 

```{r}
### Add useful columns to features and apply row filters used for all models
features_filt = features_before_on %>%
  inner_join(
    data_before %>% 
      select(person_id, screening_date, people) %>%
      unnest() %>%
      select(person_id, screening_date, race, sex, name),
    by = c("person_id","screening_date")
  ) %>%
  inner_join(features_on, by = c("person_id","screening_date")) %>%
  inner_join(
    psa_features%>%
      select(-c(p_current_age,p_prison)), 
    by = c("person_id","screening_date"))%>%
  inner_join(outcomes, by = c("person_id","screening_date")) %>%
  filter(`Risk of Recidivism_decile_score` != -1, `Risk of Violence_decile_score` != -1) %>% # Filter 1
  filter(!is.na(current_offense_date)) %>% # Filter 3
  filter(screening_date <= current_offense_date_limit) %>% # Filter 4
  mutate(recid_use = as.factor(recid), # Select recidivism or violent recidivism to use in this script 
         decile_use = `Risk of Recidivism_decile_score`) # Select recidivism or violent recidivism decile score to use in this script

## Select features and round count features
train = features_filt %>%
  transmute(
    #COMPAS Risk of Recidivism Features
    p_current_age,
    p_age_first_offense,
    p_charge,
    p_jail30 = pmin(p_jail30,5),
    p_prison = pmin(p_prison,5),
    p_probation = pmin(p_probation,5),
    
    #COMPAS Risk of violent recidivism features
    p_juv_fel_count,
    p_felprop_violarrest,
    p_murder_arrest,
    p_felassault_arrest,
    p_misdemassault_arrest,
    p_famviol_arrest,
    p_sex_arrest,
    p_weapons_arrest,
    
    #Misc Features
    p_arrest,
    p_property,
    p_traffic,
    p_drug,
    p_dui,
    p_domestic,
    p_stalking,
    p_voyeurism,
    p_fraud,
    p_stealing,
    p_trespass,
    recid_use)

# train = train[1:100,] # Comment for full training set. This is just for testing.
```


### Train xgboost

```{r}
library(xgboost)
```


```{r}
## Format for xgboost
train_xgb = xgb.DMatrix(
  "data" = train %>% select(-recid_use) %>% as.matrix(),
  "label" = train %>% select(recid_use) %>% as.matrix()
)

# Specify each parameter as a single value or a vector of values
# Each combination will be tested
param_xgb <- list(
  objective = "binary:logistic",
  eta = c(.05,.1),
  gamma = c(.5), 
  max_depth = c(2),
  min_child_weight = c(5),
  subsample = c(1),
  colsample_bytree = c(1),
  early_stopping_rounds=50
)

param_xgb = expand.grid(param_xgb) # Each row is a set of parameters to be cross validated

# Only one value for each of these parameters is allowed
setup_xgb = list(
  nrounds=10000,
  nfold=5
)
```

```{r}
set.seed(2812)
out = fit_xgb_auc(train_xgb, param_xgb, setup_xgb)

```

```{r}
out$performance
```

### Train BART

```{r}
options(java.parameters = "-Xmx2g") 
library(bartMachine)
```


```{r}
param_bart = expand.grid(list(
  num_trees = c(50, 75),
  alpha = 0.95, 
  beta = 2, 
  k = c(2), 
  q = 0.9, 
  nu = 3)
)

setup_bart = list(
  nfold=2
)

out_bart = train %>%
  mutate(y = as.factor(recid_use)) %>%
  select(-recid_use) %>%
  fit_bart_auc(param_bart,setup_bart)
```

```{r}
out_bart$performance
```


```{r}
plot_convergence_diagnostics(out_bart$mdl_best)
```

```{r}
investigate_var_importance(out_bart$mdl_best, num_replicates_for_avg = 20)
```


### Train logistic regression classifier
train_auc_mean train_auc_std test_auc_mean test_auc_std
1      0.7034106   0.004943927     0.6955958  0.004943927
```{r knitr::opts_chunk$set(message = FALSE)}
setup_glm = list(
  nfold=5
)

out_glm = train %>%
          mutate(y = as.factor(recid_use)) %>%
          select(-recid_use) %>%
          fit_glm_auc(setup_glm)

out_glm$performance
# print("accuracy is:  ")
# 1-mean(if_else(predict(out_glm$mdl_glm, type='response', newdata=train)>.5,1,0) != train$recid_use)
```

### Train LASSO 

```{r knitr::opts_chunk$set(message = FALSE)}
library(glmnet)
setup_lasso = list(
  nfold=5
)
setup = list(
  nfold=5
)

```

```{r}
out_lasso = train %>%
          mutate(y = as.factor(recid_use)) %>%
          select(-recid_use) %>%
          fit_lasso_auc(setup_glm)

out_lasso$performance

```
```{r}
  eval_varnames = c("train_auc_mean","train_auc_std","test_auc_mean","test_auc_std")
  
  performance = data.frame(
    seed = sample.int(10000, 1),
    matrix(NA,nrow=1,
           ncol=length(eval_varnames),
           dimnames=list(NULL,eval_varnames)))
  
  ## Divide into folds
  train_fold = train %>%
              mutate(y = as.factor(recid_use)) %>%
          select(-recid_use) %>%

    modelr::crossv_kfold(k = setup[["nfold"]]) %>%
    transmute(X = map(train, ~ as.matrix(select(as.data.frame(.x), -y))),
              y = map(train, ~ as.matrix(.x)$y),
              X_test = map(test, ~ as.data.frame(select(as.data.frame(.x), -y))),
              y_test = map(test, ~ as.data.frame(.x)$y))
  
  # Train model on each fold
  
  # cv =cv.glmnet()
  res = train_fold %>%
    mutate(model = map2(as.matrix(X),y, ~ glmnet(.x, y= .y ,
                                      alpha=1,
                                      family = "binomial"))) 
  
  res = res%>%
    # Compute performance statistics
    #IMPLEMENT CROSS VALIDATION??
    mutate(auc_train = pmap_dbl(list(X,y,model), ~ pROC::auc(response=..2,predictor=predict(..3,type='response',newx=..1))),
           auc_test = pmap_dbl(list(X_test,y_test,model), ~ pROC::auc(response=..2,predictor=predict(..3,type='response',newx=..1)))
    ) %>%
    
```

### Train random forest
 seed train_auc_mean train_auc_std test_auc_mean test_auc_std
1  784      0.8856112   0.003304293     0.6066994  0.003304293

```{r knitr::opts_chunk$set(message = FALSE)}
library(randomForest)
setup_rf = list(
  nfold=5, 
  seed = 784
)
```


```{r}
out_rf =  train %>%
          mutate(y = as.factor(recid_use)) %>%
          select(-recid_use) %>%
          fit_rf_auc(setup_rf)

out_rf$performance
```

### Train CART
  i_param seed train_auc_mean train_auc_std test_auc_mean test_auc_std
1       1 5163      0.6135910    0.01825666     0.6048626   0.01825666
2       2 6402      0.5416409    0.05702557     0.5361078   0.05702557
3       3 4991      0.6135910    0.01825666     0.6048626   0.01825666
4       4 1511      0.5416409    0.05702557     0.5361078   0.05702557
5       5 3526      0.6135910    0.01825666     0.6048626   0.01825666
6       6 1592      0.5416409    0.05702557     0.5361078   0.05702557
7       7  181      0.6135910    0.01825666     0.6048626   0.01825666
8       8  805      0.5416409    0.05702557     0.5361078   0.05702557
```{r}
library(rpart)

setup_cart = list(
  nfold=5
)

param_cart = expand.grid(list(
  cp=c(.01, .05), 
  minsplit = c(20,40), 
  maxdepth = c(15,30)
  )
)
```


```{r}
out_cart =  train %>%
          mutate(y = as.factor(recid_use)) %>%
          select(-recid_use) %>%
          fit_cart_auc(param_cart, setup_cart)

out_cart$performance
```



### Train SVM
0.7059543 accuracy

```{r}
param_svm = list(
  type = 'C-classification',
  cost = c(0.5,1,2),
  epsilon = .1, # This parameter isn't used for classification but need to set anyway or it will break
  gamma_scale = c(0.5,1,2)
)
# mdl_svm = fit_svm(recid_use ~ ., train, param_svm)

out_svm = train %>%
          mutate(y = as.factor(recid_use)) %>%
          select(-recid_use) %>%
          fit_svm(recid_use ~ ., train, param_svm)

out_svm$performance
print("Accuracy is: ")
1-mean(mdl_svm$fitted != train$recid_use) # ADJUST GROUP

```

